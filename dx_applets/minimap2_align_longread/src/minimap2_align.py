#!/usr/bin/env python
# blasr 3.1.0
# Generated by dx-app-wizard.
#
# Basic execution pattern: Your app will run on a single machine from
# beginning to end.
#
# See https://wiki.dnanexus.com/Developer-Portal for documentation and
# tutorials on how to modify this file.
#
# DNAnexus Python Bindings (dxpy) documentation:
#   http://autodoc.dnanexus.com/bindings/python/current/
from __future__ import print_function
import os
import multiprocessing
import dxpy
import dx_utils
import re
from collections import defaultdict

SORT_THREADS = 2
MAP_THREADS = max(multiprocessing.cpu_count()-SORT_THREADS, 2)


def _get_filenames(files):
    input = {'objects': [file['$dnanexus_link'] for file in files]}
    descriptions = dxpy.DXHTTPRequest('/system/describeDataObjects', input)

    names = [d['describe']['name'] for d in descriptions['results']]

    return names


def _get_filesizes(files):
    describe_input = {'objects': [file['$dnanexus_link'] for file in files]}
    descriptions = dxpy.DXHTTPRequest('/system/describeDataObjects', describe_input)

    sizes = [d['describe']['size'] for d in descriptions['results']]

    return sizes


def _get_movie_sizes(movies):
    '''
    :param movies: Defaultdict of movie name : [ list of files ]
    '''

    total_size = 0
    movies_and_sizes = []
    for movie, dxfiles in movies.items():
        sizes = _get_filesizes(dxfiles)
        movie_size = sum(sizes)
        total_size += movie_size
        movies_and_sizes.append((movie, movie_size))

    return movies_and_sizes, total_size


def _group_movies(dxfiles, target_size):
    '''
    :param dxfiles: List of dx files to split
    :type dxfiles: list of DXLink
    :param target_size: Target size (in bytes) of each bin
    :type target_size: Int
    :returns: Groups of files
    :rtype: List of lists of DXLink
        Takes a list of dxfiles and splits it into groups by movie attempting to have
    each group roughly the given target size worth of data.

    '''

    movies = defaultdict(list)
    fns = _get_filenames(dxfiles)
    for fn, dxfile in zip(fns, dxfiles):
        movie = re.sub('.subreads.bam', '', fn)
        movie = os.path.splitext(movie)[0]
        movies[movie].append(dxfile)

    movies_and_sizes, total_size = _get_movie_sizes(movies)

    # Now, get the splits.  We'll target each set of bam files to be a total
    # of SIZE_PER_BIN bytes.
    num_bins = total_size / (target_size * 1024 * 1024 * 1024) + 1
    groups = dx_utils.schedule_lpt(movies_and_sizes, num_bins)

    # It's conceivable that some of the splits could be empty.  We'll remove
    # those from our list.
    groups = [split for split in groups if len(split) > 0]

    splits = []
    for group in groups:
        split = []
        for movie in group:
            split.extend(movies[movie])
        splits.append(split)

    print(splits)

    return splits


def run_minimap2_subjobs(job_inputs):
    # group subjobs by filesize chunks
    files_and_filesizes = zip(job_inputs['reads'], _get_filesizes(job_inputs['reads']))
    jobs = []
    for group in dx_utils.schedule_lpt(files_and_filesizes, job_inputs['chunk_size']):
        map_reads_input = {
            'reads': group,
            'genome_fastagz': job_inputs['genome_fastagz'],
            'genome_mmi': job_inputs['genome_mmi'],
            'datatype': job_inputs['datatype']
        }
        job = dxpy.new_dxjob(map_reads_input, 'map_reads_minimap2')
        jobs.append(job)

    return jobs


def run_pbmm2_subjobs(job_inputs):
    pbi_filenames = {}
    if job_inputs.get('reads_indices'):
        filenames = _get_filenames(job_inputs['reads_indices'])

        for indx, name in enumerate(filenames):
            pbi_filenames[name] = job_inputs['reads_indices'][indx]
    else:
        pbi_filenames = {}

    # now set up and run pbmm2 subjobs for mapping reads
    # group inputs into filesizes
    jobs = []
    # set default target size to 5GB
    for group in _group_movies(job_inputs['reads'], job_inputs['chunk_size']):
        group_fns = _get_filenames(group)
        group_pbis = [pbi_filenames.get(f + '.pbi') for f in group_fns]
        map_reads_input = {
            'bam_files': group,
            'pbi_files': group_pbis,
            'genome_fastagz': job_inputs['genome_fastagz'],
            'genome_mmi': job_inputs['genome_mmi'],
            'pbbamify': job_inputs['pbbamify']
        }
        job = dxpy.new_dxjob(map_reads_input, 'map_reads_pbmm2')
        jobs.append(job)
    return jobs


@dxpy.entry_point('run_minimap_index')
def run_minimap_index(genome_fastagz):
    # load the docker images
    dx_utils.run_cmd(['docker', 'load', '-i', '/opt/minimap2_images.tar'])

    # download the reference genome
    ref_genome = dx_utils.download_and_gunzip_file(genome_fastagz)
    ofn = os.path.splitext(ref_genome)[0] + '.mmi'

    # run minimap2 index
    docker_cmd = ['docker', 'run', '-v', '/home/dnanexus:/home/dnanexus', 
                  '-w', '/home/dnanexus']
    minimap2_indx_cmd = docker_cmd + ['quay.io/biocontainers/minimap2:2.17--h84994c4_0']
    minimap2_indx_cmd += ["minimap2", "-d", ofn, ref_genome]
    dx_utils.run_cmd(minimap2_indx_cmd)

    return {'genome_mmi': dxpy.dxlink(dxpy.upload_local_file(ofn))}


@dxpy.entry_point('map_reads_pbmm2')
def map_reads_pbmm2(bam_files, pbi_files, genome_fastagz, genome_mmi, pbbamify):
    # Unpack the docker image
    dx_utils.run_cmd("docker load -i /opt/pbmm2/pbmm2_docker.tar.gz")
    # Download inputs
    reads = [dx_utils.download_and_gunzip_file(f) for f in bam_files]
    pbis = [dx_utils.download_and_gunzip_file(f) for f in pbi_files if f is not None]
    ref_genome = dx_utils.download_and_gunzip_file(genome_fastagz)
    ref_genome_mmi = dx_utils.download_and_gunzip_file(genome_mmi)

    # make sure we have full paths of inputs
    reads = [os.path.join('/home/dnanexus', r) for r in reads]
    pbis = [os.path.join('/home/dnanexus', r) for r in pbis]
    ref_genome = os.path.join('/home/dnanexus', ref_genome)
    ref_genome_mmi = os.path.join('/home/dnanexus', ref_genome_mmi)

    # use environment to set the pbmm2 variable
    docker_run = ["docker", "run",
                 # this mounts /home/dnanexus into the docker container and 
                 # sets it as the work dir
                 "-v", "/home/dnanexus:/home/dnanexus", "-w", "/home/dnanexus",
                 # this calls the container and the command we want
                 "quay.io/biocontainers/pbmm2:1.0.0--ha888412_0"]
    pbmm2_cmd = docker_run + ["pbmm2"]
    pbindex_cmd = docker_run + ["pbindex"]
    pbbamify_cmd = docker_run + ["pbbamify"]
    # run man page
    dx_utils.run_cmd(pbmm2_cmd + ['-h'])

    # Iterate over bam files
    output_ofns = []
    for bam in reads:
        prefix = re.sub("(\.subreads)?(\.bam){1}$", "", bam)
        ofn = '{0}.mapped.bam'.format(prefix)
        pb_ofn = '{0}.pb.mapped.bam'.format(prefix)
        s_pb_ofn = '{0}.pb.sorted.bam'.format(prefix)

        if bam + '.pbi' not in pbis:
            print('DNAnexus run pbindex')
            dx_utils.run_cmd(pbindex_cmd + [bam])
        else:
            print('DNAnexus not run pbindex')

        # compute memory per sorting thread
        system_memory = dx_utils.get_memory(suffix='G') - 40
        memory_per_thread = system_memory / SORT_THREADS

        # Call pbmm2 align
        pbmm2_cmd = pbmm2_cmd + ['align', str(ref_genome_mmi), str(bam), str(ofn),
        '-j', str(MAP_THREADS), 
        '--sort', '-J', str(SORT_THREADS), 
        '-m', '{0}G'.format(int(memory_per_thread)),"-N","1",
        '--log-level', 'DEBUG']
        dx_utils.run_cmd(pbmm2_cmd)
        if pbbamify:
            print('DNAnexus call pbbamify')
            pbbamify_cmd = pbbamify_cmd + ['--input=' + ofn, '--output=' + pb_ofn, ref_genome, bam]
            dx_utils.run_cmd(pbbamify_cmd)
            dx_utils.run_cmd(['rm', ofn])
            # sort
            cmd = ['samtools', 'sort', '-o', s_pb_ofn, '-@', str(multiprocessing.cpu_count()), '-T', prefix, pb_ofn]
            dx_utils.run_cmd(cmd)
            dx_utils.run_cmd(['rm', pb_ofn])
            # Create index
            cmd = ['samtools', 'index', s_pb_ofn]
            dx_utils.run_cmd(cmd)
            # append to outputs
            output_ofns.append(s_pb_ofn)

        else:
            # Create index
            cmd = ['samtools', 'index', ofn]
            dx_utils.run_cmd(cmd)
            # append to outputs
            output_ofns.append(ofn)

        # check what files are created
        dx_utils.run_cmd(['ls', '.'])

    if pbbamify:
        return {'mapped_reads': [dxpy.dxlink(dxpy.upload_local_file(ofn)) for ofn in output_ofns],
                'mapped_reads_index': [dxpy.dxlink(dxpy.upload_local_file(ofn + '.bai')) for ofn in output_ofns]}
    else:
        return {'mapped_reads': [dxpy.dxlink(dxpy.upload_local_file(s_pb_ofn)) for s_pb_ofn in output_ofns],
                'mapped_reads_index': [dxpy.dxlink(dxpy.upload_local_file(s_pb_ofn + '.bai')) for s_pb_ofn in
                                       output_ofns]}


@dxpy.entry_point('map_reads_minimap2')
def map_reads_minimap2(reads, genome_fastagz, genome_mmi, datatype):
    # load the docker images
    dx_utils.run_cmd(['docker', 'load', '-i', '/opt/minimap2_images.tar'])

    # Download inputs
    reads = [dx_utils.download_and_gunzip_file(f, skip_decompress=True) for f in reads]
    ref_genome = dx_utils.download_and_gunzip_file(genome_fastagz)
    ref_genome_mmi = dx_utils.download_and_gunzip_file(genome_mmi)

    # configure preset params
    if datatype == 'PacBio':
        preset_param = 'map-pb'
    elif datatype == 'CCS':
        preset_param = 'asm20'
    else:
        preset_param = 'map-ont'

    docker_cmd = ['docker', 'run', '-v', '/home/dnanexus:/home/dnanexus', 
                  '-w', '/home/dnanexus']
    minimap2_docker_cmd = docker_cmd + ['quay.io/biocontainers/minimap2:2.17--h84994c4_0']
    # Run sambamba with -i flag, meaning it can read from stdin
    sambamba_docker_cmd = docker_cmd + ['-i', 
                        'quay.io/biocontainers/sambamba:0.6.8--h682856c_1']
    # Iterate over reads files
    output_ofns = []
    for read in reads:
        output_prefix = re.sub("\.(fastq|fasta|fa|fq){1}(.gz)?$", "", read)
        ofn = '{0}.mapped.bam'.format(output_prefix)

        # Call minimap2
        minimap2_cmd = minimap2_docker_cmd + ['minimap2', '-ax', preset_param, ref_genome_mmi, read]
        view_cmd = sambamba_docker_cmd + ['sambamba', 'view', '--sam-input', '--format=bam',
                    '--compression-level=0', '/dev/stdin']
        sort_cmd = sambamba_docker_cmd + ['sambamba', 'sort', '-m', 
                    '{0}G'.format(int(dx_utils.get_memory(suffix='G'))), '-o',
                    ofn, '-t', str(multiprocessing.cpu_count()), '/dev/stdin']
        dx_utils.run_pipe(minimap2_cmd, view_cmd, sort_cmd)

        # index
        dx_utils.run_cmd(sambamba_docker_cmd + ['sambamba', 'index', ofn])
        # append to outputs
        output_ofns.append(ofn)
    return {'mapped_reads': [dxpy.dxlink(dxpy.upload_local_file(ofn)) for ofn in output_ofns],
            'mapped_reads_index': [dxpy.dxlink(dxpy.upload_local_file(ofn + '.bai')) for ofn in output_ofns]}


@dxpy.entry_point('main')
def main(**job_inputs):
    # If we weren't provided a mmi index for the reference, generate it.
    if 'genome_mmi' not in job_inputs:
        mmi_input = {'genome_fastagz': job_inputs['genome_fastagz']}
        minimap_index_job = dxpy.new_dxjob(mmi_input, 'run_minimap_index')
        job_inputs['genome_mmi'] = minimap_index_job.get_output_ref('genome_mmi')
    output = {'genome_mmi': job_inputs['genome_mmi']}

    # check if we're dealing with pacbio or ONT reads and what the filetype is
    datatype = job_inputs['datatype']
    one_reads_file = dxpy.DXFile(job_inputs['reads'][0]).describe()['name']
    try:
        file_ext = re.search("(bam|fastq|fasta|fa|fq){1}(.gz)?$", one_reads_file, flags=re.I).group(1).lower()
    except AttributeError:
        raise dxpy.AppError("Unknown filetype extension supplied.")

    if file_ext == 'bam':
        # input bam files must be pacbio raw reads
        if datatype == 'ONT':
            raise dxpy.AppError("Invalid file input for provided datatype.")

        # for bam input, run jobs using pbmm2
        jobs = run_pbmm2_subjobs(job_inputs)

    else:
        # for fasta and fastq inputs, run jobs using native minimap2
        if job_inputs['pbbamify']:
            print('WARNING: The "Run pbbamify" option is only valid for BAM input')
        jobs = run_minimap2_subjobs(job_inputs)

    output['bam_files'] = [j.get_output_ref('mapped_reads') for j in jobs]
    output['bai_files'] = [j.get_output_ref('mapped_reads_index') for j in jobs]

    return output

dxpy.run()
