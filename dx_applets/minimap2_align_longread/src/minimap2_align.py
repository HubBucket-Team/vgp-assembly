#!/usr/bin/env python
# blasr 3.1.0
# Generated by dx-app-wizard.
#
# Basic execution pattern: Your app will run on a single machine from
# beginning to end.
#
# See https://wiki.dnanexus.com/Developer-Portal for documentation and
# tutorials on how to modify this file.
#
# DNAnexus Python Bindings (dxpy) documentation:
#   http://autodoc.dnanexus.com/bindings/python/current/
from __future__ import print_function
import os
import multiprocessing
import dxpy
import dx_utils
import re
import tempfile
from collections import defaultdict

SORT_THREADS = 2
MAP_THREADS = max(multiprocessing.cpu_count()-SORT_THREADS, 2)


def _get_filenames(files):
    input = {'objects': [file['$dnanexus_link'] for file in files]}
    descriptions = dxpy.DXHTTPRequest('/system/describeDataObjects', input)

    names = [d['describe']['name'] for d in descriptions['results']]

    return names


def _get_filesizes(files):
    describe_input = {'objects': [file['$dnanexus_link'] for file in files]}
    descriptions = dxpy.DXHTTPRequest('/system/describeDataObjects', describe_input)

    sizes = [d['describe']['size'] for d in descriptions['results']]

    return sizes


def _get_movie_sizes(movies):
    '''
    :param movies: Defaultdict of movie name : [ list of files ]
    '''

    total_size = 0
    movies_and_sizes = []
    for movie, dxfiles in movies.items():
        sizes = _get_filesizes(dxfiles)
        movie_size = sum(sizes)
        total_size += movie_size
        movies_and_sizes.append((movie, movie_size))

    return movies_and_sizes, total_size


def _group_movies(dxfiles, target_size):
    '''
    :param dxfiles: List of dx files to split
    :type dxfiles: list of DXLink
    :param target_size: Target size (in bytes) of each bin
    :type target_size: Int
    :returns: Groups of files
    :rtype: List of lists of DXLink
        Takes a list of dxfiles and splits it into groups by movie attempting to have
    each group roughly the given target size worth of data.

    '''

    movies = defaultdict(list)
    fns = _get_filenames(dxfiles)
    for fn, dxfile in zip(fns, dxfiles):
        movie = re.sub('.subreads.bam', '', fn)
        movie = os.path.splitext(movie)[0]
        movies[movie].append(dxfile)

    movies_and_sizes, total_size = _get_movie_sizes(movies)

    # Now, get the splits.  We'll target each set of bam files to be a total
    # of SIZE_PER_BIN bytes.
    num_bins = total_size / (target_size * 1024 * 1024 * 1024) + 1
    groups = dx_utils.schedule_lpt(movies_and_sizes, num_bins)

    # It's conceivable that some of the splits could be empty.  We'll remove
    # those from our list.
    groups = [split for split in groups if len(split) > 0]

    splits = []
    for group in groups:
        split = []
        for movie in group:
            split.extend(movies[movie])
        splits.append(split)

    print(splits)

    return splits


def run_minimap2_subjobs(job_inputs):
    # group subjobs by filesize chunks
    files_and_filesizes = zip(job_inputs['reads'], _get_filesizes(job_inputs['reads']))
    jobs = []
    for group in dx_utils.schedule_lpt(files_and_filesizes, job_inputs['chunk_size']):
        map_reads_input = {
            'reads': group,
            'genome_fastagz': job_inputs['genome_fastagz'],
            'genome_mmi': job_inputs['genome_mmi'],
            'datatype': job_inputs['datatype']
        }
        job = dxpy.new_dxjob(map_reads_input, 'map_reads_minimap2')
        jobs.append(job)

    return jobs


def run_pbmm2_subjobs(job_inputs):
    pbi_filenames = {}
    if job_inputs.get('reads_indices'):
        filenames = _get_filenames(job_inputs['reads_indices'])

        for indx, name in enumerate(filenames):
            pbi_filenames[name] = job_inputs['reads_indices'][indx]
    else:
        pbi_filenames = {}

    # now set up and run pbmm2 subjobs for mapping reads
    # group inputs into filesizes
    jobs = []
    # set default target size to 5GB
    for group in _group_movies(job_inputs['reads'], job_inputs['chunk_size']):
        group_fns = _get_filenames(group)
        group_pbis = [pbi_filenames.get(f + '.pbi') for f in group_fns]
        map_reads_input = {
            'bam_files': group,
            'pbi_files': group_pbis,
            'genome_fastagz': job_inputs['genome_fastagz'],
            'genome_mmi': job_inputs['genome_mmi']
        }
        job = dxpy.new_dxjob(map_reads_input, 'map_reads_pbmm2')
        jobs.append(job)
    return jobs


@dxpy.entry_point('run_minimap_index')
def run_minimap_index(genome_fastagz):
    ref_genome = dx_utils.download_and_gunzip_file(genome_fastagz)
    ofn = os.path.splitext(ref_genome)[0] + '.mmi'

    minimap2_indx_cmd = ["minimap2", "-d", ofn, ref_genome]
    dx_utils.run_cmd(minimap2_indx_cmd)

    return {'genome_mmi': dxpy.dxlink(dxpy.upload_local_file(ofn))}


@dxpy.entry_point('map_reads_pbmm2')
def map_reads_pbmm2(bam_files, pbi_files, genome_fastagz, genome_mmi):
    # Download inputs
    reads = [dx_utils.download_and_gunzip_file(f) for f in bam_files]
    pbis = [dx_utils.download_and_gunzip_file(f) for f in pbi_files]
    ref_genome = dx_utils.download_and_gunzip_file(genome_fastagz)
    ref_genome_mmi = dx_utils.download_and_gunzip_file(genome_mmi)

    # find out what environment we're dealing with
    print(os.environ)

    # use environment to set path
    pbmm2_env = {
        "PATH":  os.environ['PATH'] + os.pathsep + '/anaconda/bin/',
        "SHELL": '/bin/bash',
        "USER": 'dnanexus'
                }

    # create bam dataset
    with dx_utils.set_env(**pbmm2_env):
        # Iterate over bam files
        output_ofns = []
        for bam in reads:
            prefix = re.sub("(\.subreads)?(\.bam){1}$", "", bam)
            ofn = '{0}.mapped.bam'.format(prefix)
            if bam + '.pbi' not in pbis:
                dx_utils.run_cmd('pbindex {0}'.format(bam))

            dx_utils.run_cmd(['pbmm2', 'align', '--help'])
            # Call minimap2
            # compute memory per sorting thread
            system_memory = dx_utils.get_memory(suffix='G') - 40
            memory_per_thread = system_memory / SORT_THREADS
            pbmm2_cmd = ['/anaconda/bin/pbmm2', 'align', str(ref_genome_mmi), str(bam), str(ofn),
            '-j', str(MAP_THREADS), 
            '--sort', '-J', str(SORT_THREADS), 
            '-m', '{0}G'.format(int(memory_per_thread)),
            '--log-level', 'DEBUG']

            # there's some env variable that is causing pbmm2 to misbehave
            # when run from a python shell. Therefore, write the command to a
            # temp .sh file and execute it that way.
            tmp_cmd = tempfile.NamedTemporaryFile(mode='w', suffix='.sh', delete=False)
            tmp_cmd.write(' '.join(pbmm2_cmd))
            tmp_cmd.close()
            print('Executing: {0}'.format(' '.join(pbmm2_cmd)))
            dx_utils.run_cmd(['sudo', 'bash', tmp_cmd.name])

            # Create index
            cmd = ['samtools', 'index', ofn]
            dx_utils.run_cmd(cmd)

            # append to outputs
            output_ofns.append(ofn)
    return {'mapped_reads': [dxpy.dxlink(dxpy.upload_local_file(ofn)) for ofn in output_ofns],
            'mapped_reads_index': [dxpy.dxlink(dxpy.upload_local_file(ofn + '.bai')) for ofn in output_ofns]}


@dxpy.entry_point('map_reads_minimap2')
def map_reads_minimap2(reads, genome_fastagz, genome_mmi, datatype):
    # Download inputs
    reads = [dx_utils.download_and_gunzip_file(f, skip_decompress=True) for f in reads]
    ref_genome = dx_utils.download_and_gunzip_file(genome_fastagz)
    ref_genome_mmi = dx_utils.download_and_gunzip_file(genome_mmi)

    # configure preset params
    if datatype == 'PacBio':
        preset_param = 'map-pb'
    else:
        preset_param = 'map-ont'

    # Iterate over reads files
    output_ofns = []
    for read in reads:
        output_prefix = re.sub("\.(fastq|fasta|fa|fq){1}(.gz)?$", "", read)
        ofn = '{0}.mapped.bam'.format(output_prefix)
        # Get help info
        dx_utils.run_cmd(['minimap2', '-h'])
        # Call minimap2
        minimap2_cmd = ['minimap2', '-ax', preset_param, ref_genome, read]
        view_cmd = ['sambamba', 'view', '--sam-input', '--format=bam',
                    '--compression-level=0', '/dev/stdin']
        sort_cmd = ['sambamba', 'sort', '-m', 
                    '{0}G'.format(int(dx_utils.get_memory(suffix='G'))), '-o',
                    ofn, '-t', str(multiprocessing.cpu_count()), '/dev/stdin']
        dx_utils.run_pipe(minimap2_cmd, view_cmd, sort_cmd)

        # index
        dx_utils.run_cmd(['sambamba', 'index', ofn])
        # append to outputs
        output_ofns.append(ofn)
    return {'mapped_reads': [dxpy.dxlink(dxpy.upload_local_file(ofn)) for ofn in output_ofns],
            'mapped_reads_index': [dxpy.dxlink(dxpy.upload_local_file(ofn + '.bai')) for ofn in output_ofns]}


@dxpy.entry_point('main')
def main(**job_inputs):
    # If we weren't provided a mmi index for the reference, generate it.
    if 'genome_mmi' not in job_inputs:
        mmi_input = {'genome_fastagz': job_inputs['genome_fastagz']}
        minimap_index_job = dxpy.new_dxjob(mmi_input, 'run_minimap_index')
        job_inputs['genome_mmi'] = minimap_index_job.get_output_ref('genome_mmi')
    output = {'genome_mmi': job_inputs['genome_mmi']}

    # check if we're dealing with pacbio or ONT reads and what the filetype is
    datatype = job_inputs['datatype']
    one_reads_file = dxpy.DXFile(job_inputs['reads'][0]).describe()['name']
    try:
        file_ext = re.search("(fastq|fasta|fa|fq){1}(.gz)?$", one_reads_file, flags=re.I).group(1).lower()
    except AttributeError:
        raise dxpy.AppError("Invalid filetype extension supplied.")

    # for fasta and fastq inputs, run jobs using native minimap2
    jobs = run_minimap2_subjobs(job_inputs)

    output['bam_files'] = [j.get_output_ref('mapped_reads') for j in jobs]
    output['bai_files'] = [j.get_output_ref('mapped_reads_index') for j in jobs]

    return output

dxpy.run()
