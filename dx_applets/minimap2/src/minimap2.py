#!/usr/bin/env python
# minimap2 0.0.1
# Generated by dx-app-wizard.
#
# Basic execution pattern: Your app will run on a single machine from
# beginning to end.
#
# See https://wiki.dnanexus.com/Developer-Portal for documentation and
# tutorials on how to modify this file.
#
# DNAnexus Python Bindings (dxpy) documentation:
#   http://autodoc.dnanexus.com/bindings/python/current/
from __future__ import print_function
import dxpy
import os
import glob
import multiprocessing
import dx_utils


def _get_filesizes(files):
    input = {'objects': [file['$dnanexus_link'] for file in files]}
    descriptions = dxpy.DXHTTPRequest('/system/describeDataObjects', input)

    sizes = [d['describe']['size'] for d in descriptions['results']]

    return sizes


def _count_lines(file):
    if file.endswith('.gz'):
        cmd = 'zcat -dc {0} | wc -l'.format(file)
    else:
        cmd = 'wc -l {0}'.format(file)

    num_lines = int(dx_utils.run_cmd(cmd, returnOutput=True))
    return num_lines


@dxpy.entry_point('map_splits')
def map_splits(R1_reads, R2_reads, args):
    jobs = []

    if R2_reads:
        for R1, R2 in zip(R1_reads, R2_reads):
            job_input = args.copy()
            job_input['reads_fastqgz'] = R1
            job_input['reads2_fastqgz'] = R2

            job = dxpy.new_dxjob(job_input, 'run_minimap')
            jobs.append(job)

    else:
        for R1 in R1_reads:
            job_input = args.copy()
            job_input['reads_fastqgz'] = R1
            job_input['reads2_fastqgz'] = None

            job = dxpy.new_dxjob(job_input, 'run_minimap')
            jobs.append(job)

    merge_input = {'sorted_bams': [j.get_output_ref('sorted_bam') for j in jobs]}
    merge_job = dxpy.new_dxjob(merge_input, 'merge')

    output = {}
    output['sorted_bam'] = [merge_job.get_output_ref('sorted_bam')]

    return output


@dxpy.entry_point('merge')
def merge(bams, bais):
    bams = [dx_utils.download_and_gunzip(bam) for bam in bams]
    output_prefix = os.path.commonprefix(bams)

    cmd = 'samtools merge -@ {nproc} {output_prefix}.merged.bam {sorted_bams}'.format(
        nproc=multiprocessing.cpu_count(), output_prefix=output_prefix, sorted_bams=' '.join(bams))
    dx_utils.run_cmd(cmd)

    cmd = 'samtools index {prefix}.merged.bam'.format(output_prefix)
    dx_utils.run_cmd(cmd)

    output = {'sorted_bam': dxpy.dxlink(dxpy.upload_local_file(output_prefix + '.merged.bam'))}

    return output


@dxpy.entry_point('split_fastq')
def split_fastq(fastq, num_chunks):
    # split the file into given number of chunks
    # this might not be the fastest way to do it but let's go with it for now
    fastq = dx_utils.download_and_gunzip_file(fastq, skip_decompress=True)
    num_lines = _count_lines(fastq)
    reads_per_chunk = int((num_lines / 4) / num_chunks) + 1
    lines_per_chunk = reads_per_chunk * 4

    output_prefix = fastq.replace('.fastq.gz', '_part')
    cmd = 'zcat {fastq} | split -l {num_lines} -d - {prefix} --additional-suffix=".fastq" --filter=\'gzip > $FILE.gz\''.format(
        fastq=fastq, num_lines=lines_per_chunk, prefix=output_prefix)
    dx_utils.run_cmd(cmd)

    dx_utils.run_cmd('ls .')
    output_splits = glob.glob('*_part*.fastq.gz')
    output = {'splits': [dxpy.dxlink(dxpy.upload_local_file(f)) for f in output_splits]}

    return output

@dxpy.entry_point('run_minimap')
def run_minimap(**job_inputs):
    # download genome index files
    genome_fastagz = dx_utils.download_and_gunzip_file(job_inputs['genome_fastagz'])
    # download reads
    R1_fastq = dx_utils.download_and_gunzip_file(job_inputs['reads_fastqgz'], skip_decompress=True)

    # Make fifo for output bam and attach upload process
    os.mkfifo('{0}.bam'.format(output_prefix))
    upload_proc = subprocess.Popen(['dx', 'upload', '--brief', '{0}.bam'.format(output_prefix)], stdout=subprocess.PIPE)

    if job_inputs.get('reads2_fastqgz'):
        R2_fastq = dx_utils.download_and_gunzip_file(
            job_inputs['reads2_fastqgz'], skip_decompress=True)
        input = [R1_fastq, R2_fastq]
        output_prefix = os.path.commonprefix([R1_fastq, R2_fastq])
    else:
        input = [R1_fastq]
        output_prefix = R1_fastq.split('.')[0]

    # set options
    minimap2_cmd = ['minimap2', '-ax']
    if sequencing_technology == 'pacbio':
        minimap2_cmd += ['map-pb', '-L']
    elif sequencing_technology == 'ont':
        minimap2_cmd += ['map-ont', '-L']
    else:
        minimap2_cmd += ['sr']

    minimap2_cmd.append(genome_fastagz)
    minimap2_cmd.extend(input)

    if sequencing_technology == 'illumina':
        bamsormadup_cmd = ['bamsormadup', 'SO=coordinate', 'threads={0}'.format(multiprocessing.cpu_count()), 
            'inputformat=sam', 'indexfilename="{0}".bam.bai'.format(output_prefix)]
        print(_list2cmdlines_pipe(minimap2_cmd, bamsormadup_cmd))
        # Now actually make the calls
        minimap_proc = subprocess.Popen(minimap2_cmd, stdout=subprocess.PIPE)
        with open('{0}.bam'.format(output_prefix), 'w') as fh:
            subprocess.check_call(bamsormadup_cmd, stdin=minimap_proc.stdout, stdout=fh)
    else:
        view_cmd = ['sambamba', 'view', '--sam-input', '--format=bam', '--compression-level=0', '/dev/stdin']
        sort_cmd = ['sambamba', 'sort', '-m', '{0}G'.format(_get_mem_in_gb()), '-o', 
            '{0}.bam'.format(output_prefix), '-t', str(multiprocessing.cpu_count()), '/dev/stdin']
        print(_list2cmdlines_pipe(minimap2_cmd, view_cmd, sort_cmd))
        # Now actually make the calls
        minimap_proc = subprocess.Popen(minimap2_cmd, stdout=subprocess.PIPE)
        sambamba_view_proc = subprocess.Popen(view_cmd, stdin=minimap_proc.stdout, stdout=subprocess.PIPE)
        subprocess.check_call(sort_cmd, stdin=sambamba_view_proc.stdout)

    minimap_proc.communicate()
    bam_fid, err = upload_proc.communicate()
    # Now upload the output
    output = {}
    output['sorted_bam'] = dxpy.dxlink(bam_fid.strip())

    return output


@dxpy.entry_point('main')
def main(**job_inputs):
    if 'reads2_fastqgz' in job_inputs:
        reads = [job_inputs['reads_fastqgz'], job_inputs['reads2_fastqgz']]

    else:
        reads = [job_inputs['reads_fastqgz']]

    # convert chunk size input (in GB) to bytes
    chunk_size = int(job_inputs.get('chunk_size', 100000) * 1024**3)

    file_sizes = _get_filesizes(reads)
    if max(file_sizes) > chunk_size:
        # figure out how many chunks to split files into
        num_chunks = int(max(file_sizes) / chunk_size)

        R1_split_job = dxpy.new_dxjob({'fastq': job_inputs['reads_fastqgz'],
                                       'num_chunks': num_chunks}, 'split_fastq')

        R1_reads = R1_split_job.get_output_ref('splits')

        if job_inputs.get('reads2_fastqgz'):
            R2_split_job = dxpy.new_dxjob({'fastq': job_inputs['reads2_fastqgz'],
                                           'num_chunks': num_chunks}, 'split_fastq')
            R2_reads = R2_split_job.get_output_ref('splits')

        else:
            R2_reads = None

        map_job_input = {'R1_reads': R1_reads,
                         'R2_reads': R2_reads,
                         'args': job_inputs.copy()}
        job = dxpy.new_dxjob(map_job_input, 'map_splits')

    else:
        job_input = job_inputs.copy()
        job = dxpy.new_dxjob(job_input, 'run_minimap')

    output = {'sorted_bam': job.get_output_ref('sorted_bam')}
    return output

dxpy.run()
